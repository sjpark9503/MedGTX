{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Count labels in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "db_type = 'NoKGenc'\n",
    "db_name = 'px'\n",
    "size = 1000\n",
    "SPLIT = ['train','valid','test']\n",
    "labels = list()\n",
    "\n",
    "for split in SPLIT:\n",
    "    db_new = torch.load(f'/home/ssbae/bae/workspace/kgtxt/kg_txt_multimodal/gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "    for label in tqdm(db_new['label']):\n",
    "        labels+=label\n",
    "label_count = dict()\n",
    "for label in tqdm(labels):\n",
    "    if label not in label_count:\n",
    "        label_count[label]=0\n",
    "    label_count[label]+=1\n",
    "label_count.pop(-100)\n",
    "print(len(label_count))\n",
    "\n",
    "# Prepare essential files\n",
    "NUM_SPECIAL_TOKENS = 3\n",
    "id2label = torch.load(f'/home/ssbae/bae/workspace/kgtxt/kg_txt_multimodal/gtx/data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "# id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'/home/ssbae/bae/workspace/kgtxt/kg_txt_multimodal/gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "# label2entity = {k:id2entity[v] for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_label_count = label_count.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4,5])\n",
    "b = torch.tensor([1,2,3,4,5])\n",
    "a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "db_type = 'UnifiedUniKGenc'\n",
    "db_name = 'dx,prx'\n",
    "size = 2000\n",
    "id2label = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/node2uninode')\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "lower = 1000\n",
    "upper = 1e10\n",
    "NUM_SPECIAL_TOKENS = 3\n",
    "db_name = 'px'\n",
    "\n",
    "# Filter out labels\n",
    "if db_name == 'px':\n",
    "    triples = [x.split() for x in open(os.path.join(f'../gtx/data/{db_name}','train2id.txt')).read().splitlines()[1:]]\n",
    "    drugname_nodes = set([int(t)+NUM_SPECIAL_TOKENS for h,t,r in triples if int(r) in [3,5,6]])\n",
    "    label_count = {k:v for k,v in ori_label_count.items() if label2id[k] in drugname_nodes}\n",
    "filtered_label_counts = {k:v for k,v in label_count.items() if (v>lower) and (v<upper)}\n",
    "num_labels = np.array([count for count in filtered_label_counts.values()])\n",
    "print(len(num_labels))\n",
    "\n",
    "# Plot histogram of label distribution\n",
    "plt.hist(num_labels, bins=np.arange(num_labels.min(), num_labels.max() + 1000, 1000), rwidth=0.7,\n",
    "         density=True, cumulative=True,\n",
    "         range=(num_labels.min(), num_labels.max()))\n",
    "plt.title(f'l:{lower},u:{upper},#labels:{len(num_labels)}')\n",
    "# plt.savefig(f'data/{db_name}/edlabel_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Generate dictionary for label --> AdmPred labels\n",
    "# ap_label = {v:k for k,v in enumerate(label_count)}\n",
    "# torch.save(ap_label,f'data/{db_name}/label2edlabel')\n",
    "# print([label2entity[k] for k in torch.load(f'data/{db_name}/label2edlabel').keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.eye(3)\n",
    "b = torch.rand(3,4)\n",
    "for _a,_b in zip(a,b):\n",
    "    print(len(_a))\n",
    "    print(_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert DB for AdmPred task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "db_name = 'dx,prx'\n",
    "size = 2000\n",
    "DB_type = ['NoKGenc','UniKGenc','UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']\n",
    "\n",
    "# # Prepare essential files\n",
    "# NUM_SPECIAL_TOKENS = 3\n",
    "# id2label = torch.load(f'data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "# label2id = {v:k for k,v in id2label.items()}\n",
    "# id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "# label2entity = {k:id2entity[v] for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adm2label = torch.load(f\"../gtx/data/{'px' if db_name=='dx,prx' else 'dx,prx'}/adm2label\")\n",
    "label2aplabel = torch.load(f\"../gtx/data/{'px' if db_name=='dx,prx' else 'dx,prx'}/label2aplabel\")\n",
    "\n",
    "for db_type in DB_type:\n",
    "    # Prepare essential files\n",
    "    if \"Unified\" not in db_type:\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2label = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "        label2id = {v:k for k,v in id2label.items()}\n",
    "        id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "        label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    else:\n",
    "        id2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/unified_node').items()}\n",
    "\n",
    "    if db_type == 'NoKGenc':\n",
    "        global_label = list()\n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['label_mask','rc_index']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'NoKGenc':\n",
    "                if id2entity[_input[1]] in adm2label:\n",
    "                    if any([True if label in label2aplabel else False for label in adm2label[id2entity[_input[1]]]]):\n",
    "                        for k in db_new:\n",
    "                            if k not in ['label','label_mask','rc_index']:\n",
    "                                db_new[k].append(db[k][in_db_idx])\n",
    "                        aplabel = [label2aplabel[k] for k in adm2label[id2entity[_input[1]]] if k in label2aplabel]\n",
    "                        db_new['label'].append(aplabel)\n",
    "                        global_label.append(aplabel)\n",
    "                    else:\n",
    "                        global_label.append(None)\n",
    "                else:\n",
    "                    global_label.append(None)\n",
    "            else:\n",
    "                if global_label[idx] is not None:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(global_label[idx])\n",
    "            idx += 1\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        os.makedirs(f'../gtx/data/adm/{db_name}_{size}/{db_name}_{db_type}/{split}')\n",
    "        torch.save(db_new,f'../gtx/data/adm/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "\n",
    "                            \n",
    "                \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Adm Pred DB\n",
    "labels = list()\n",
    "db_new = torch.load(f'data/adm/{db_name}_{size}/{db_name}_NoKGenc/train/db')\n",
    "num_samples = len(db_new['label'])\n",
    "for label in tqdm(db_new['label']):\n",
    "    labels+=label\n",
    "\n",
    "# Get sample frequency based class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "aplabel2weight = compute_class_weight(class_weight = \"balanced\" , \n",
    "                     classes=np.unique(labels), \n",
    "                     y = labels)\n",
    "torch.save(aplabel2weight,f'data/{db_name}/adm_class_weight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convert DB for ErrorDetection for Px,Dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load D_ICD files\n",
    "import torch\n",
    "import pandas as pd\n",
    "d_proc = pd.read_csv(\"../gtx/data/mimic_data/D_ICD_PROCEDURES.csv\")\n",
    "d_diag = pd.read_csv(\"../gtx/data/mimic_data/D_ICD_DIAGNOSES.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(code, is_diag):\n",
    "    \"\"\"\n",
    "        Put a period in the right place because the MIMIC-3 data files exclude them.\n",
    "        Generally, procedure codes have dots after the first two digits, \n",
    "        while diagnosis codes have dots after the first three digits.\n",
    "    \"\"\"\n",
    "    code = ''.join(code.split('.'))\n",
    "    if is_diag:\n",
    "        if code.startswith('E'):\n",
    "            if len(code) > 4:\n",
    "                code = code[:4] + '.' + code[4:]\n",
    "        else:\n",
    "            if len(code) > 3:\n",
    "                code = code[:3] + '.' + code[3:]\n",
    "    else:\n",
    "        code = code[:2] + '.' + code[2:]\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_diag['absolute_code'] = d_diag.apply(lambda row: str(reformat(str(row[1]), True)), axis=1)\n",
    "d_proc['absolute_code'] = d_proc.apply(lambda row: str(reformat(str(row[1]), False)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_diag['code_name'] = d_diag.apply(lambda row: str(row[3]).lower().strip(), axis=1)\n",
    "d_proc['code_name'] = d_proc.apply(lambda row: str(row[3]).lower().strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_diag_dict = d_diag[['absolute_code','code_name']].to_dict()\n",
    "d_code2name = {d_diag_dict['absolute_code'][idx]:d_diag_dict['code_name'][idx] for idx in range(len(d_diag))}\n",
    "d_name2codecat = {d_diag_dict['code_name'][idx]:dict([('large',d_diag_dict['absolute_code'][idx].split(\".\")[0]),('small',d_diag_dict['absolute_code'][idx].split(\".\")[-1] if len(d_diag_dict['absolute_code'][idx].split(\".\"))>1 else \"\")])  for idx in range(len(d_diag))}\n",
    "d_proc_dict = d_proc[['absolute_code','code_name']].to_dict()\n",
    "p_code2name = {d_proc_dict['absolute_code'][idx]:d_proc_dict['code_name'][idx] for idx in range(len(d_proc))}\n",
    "p_name2codecat = {d_proc_dict['code_name'][idx]:dict([('large',d_proc_dict['absolute_code'][idx].split(\".\")[0]),('small',d_proc_dict['absolute_code'][idx].split(\".\")[-1] if len(d_diag_dict['absolute_code'][idx].split(\".\"))>1 else \"\")])  for idx in range(len(d_proc))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2name = dict()\n",
    "for d in [d_code2name, p_code2name]:\n",
    "    for k,v in d.items():\n",
    "        code2name[k.lower()] = f'{v}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2codecat = dict()\n",
    "for d in [d_name2codecat, p_name2codecat]:\n",
    "    for k,v in d.items():\n",
    "        v['large'] = v['large'].lower()\n",
    "        name2codecat[f'{k}'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2name_abs = dict()\n",
    "for d in [d_code2name, p_code2name]:\n",
    "    for k,v in d.items():\n",
    "        code2name_abs[k.lower()] = f'{v}'\n",
    "codebook = dict()\n",
    "for k in code2name_abs:\n",
    "    try:\n",
    "        large, small = k.split(\".\")\n",
    "    except:\n",
    "        large, small = k, \"\"\n",
    "    if large not in codebook:\n",
    "        codebook[large] = list()\n",
    "    codebook[large].append(small)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "db_name = 'dx,prx'\n",
    "size = 2000\n",
    "DB_type = ['NoKGenc','UniKGenc','UnifiedNoKGenc','UnifiedUniKGenc']#,'UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_type = 'UnifiedNoKGenc'\n",
    "uniid2entity = {v:k.split('\\t')[0].split('^^')[0].strip('\"').replace('\\\\\"','\"') for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_UnifiedNoKGenc/unified_node').items()}\n",
    "tot_amt = dict()\n",
    "for split in SPLIT:\n",
    "    db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "    for idx in range(len(db['input'])):\n",
    "        ids = [x for x in db['input'][idx] if x>7]\n",
    "        for _id in ids:\n",
    "            if _id not in tot_amt:\n",
    "                tot_amt[_id] = 0\n",
    "            tot_amt[_id]+=1\n",
    "code_frequency = {'.'.join([name2codecat[uniid2entity[k]]['large'],name2codecat[uniid2entity[k]]['small']]):1/v for k,v in tot_amt.items()}\n",
    "uniid2entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(code, is_diag):\n",
    "    \"\"\"\n",
    "        Put a period in the right place because the MIMIC-3 data files exclude them.\n",
    "        Generally, procedure codes have dots after the first two digits, \n",
    "        while diagnosis codes have dots after the first three digits.\n",
    "    \"\"\"\n",
    "    code = ''.join(code.split('.'))\n",
    "    if is_diag:\n",
    "        if code.startswith('e'):\n",
    "            if len(code) > 4:\n",
    "                code = code[:4] + '.' + code[4:]\n",
    "        else:\n",
    "            if len(code) > 3:\n",
    "                code = code[:3] + '.' + code[3:]\n",
    "    else:\n",
    "        code = code[:2] + '.' + code[2:]\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_input_and_generate_label(inputs, mode, id2entity, entity2id, code_frequency=None):\n",
    "    if code_frequency is None:\n",
    "        raise ValueError(\"Must turn on non-uniform sampling\")\n",
    "    try:\n",
    "        inputs=np.array(inputs)\n",
    "        inputs_ori = inputs.copy()\n",
    "        input_entities = np.array([id2entity[x] if x in id2entity else x for x in inputs])\n",
    "\n",
    "        if mode == 's':\n",
    "            codes = list()\n",
    "            f = list()\n",
    "            for x in input_entities:\n",
    "                if (\"icd9_code\" in x) and (x not in codes):\n",
    "                    is_diag = True if 'diag' in x.split('/')[1] else False\n",
    "                    code = name2codecat[code2name[reformat(x.split('/')[-1].strip(\">\"),is_diag = is_diag)]]\n",
    "                    small_cat_length = len(codebook[code['large']])\n",
    "                    if small_cat_length>=2:\n",
    "                        codes.append(x)\n",
    "                        f.append(code_frequency[\".\".join([code['large'],code['small']])])\n",
    "            p = [x/sum(f) for x in f]\n",
    "            corruption_target_codes = choice(codes,size=max(int(len(codes)*0.25),1), replace=False, p=p) \n",
    "            corruption_targets_idx = np.array([np.where(input_entities==code)[0][0] for code in corruption_target_codes])\n",
    "            for corruption_target in corruption_targets_idx:\n",
    "                code_entity = input_entities[corruption_target].split(\"/\")\n",
    "                header = '/'.join(code_entity[:-1])\n",
    "                is_diag = True if 'diag' in header else False\n",
    "                code = reformat(code_entity[-1].strip(\">\"),is_diag=is_diag)\n",
    "\n",
    "                target_literal_idx = np.where(entity2id[code2name[code]] == inputs)[0][0]\n",
    "\n",
    "                icd_code = code2name[code]\n",
    "                codecat = name2codecat[icd_code]\n",
    "                large_cat, small_cat = codecat['large'], codecat['small']\n",
    "                small_cat_lists = codebook[large_cat].copy()\n",
    "\n",
    "                small_cat_lists.remove(small_cat)\n",
    "                for _ in range(50):\n",
    "                    corrupted_small_cat = choice(small_cat_lists)\n",
    "                    if code2name[\".\".join([large_cat,corrupted_small_cat])] in entity2id:\n",
    "                        ERROR_FLAG=False\n",
    "                        break\n",
    "                    else:\n",
    "                        ERROR_FLAG = True\n",
    "                if ERROR_FLAG:\n",
    "                    raise ValueError()\n",
    "                inputs[target_literal_idx] = entity2id[code2name[\".\".join([large_cat,corrupted_small_cat])]]\n",
    "                inputs[corruption_target] = entity2id['/'.join([header,\"\".join([large_cat,corrupted_small_cat])])+\">\"]\n",
    "            labels = ~(inputs_ori==inputs)\n",
    "    except:\n",
    "        inputs = None\n",
    "        labels = None\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for db_type in DB_type:\n",
    "    print(db_type)\n",
    "    if db_type == 'NoKGenc':\n",
    "        global_sample = list()\n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        # Prepare essential files\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0].strip('\"').replace('\\\\\"','\"') for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "        id2entity[0]='PAD'\n",
    "        id2entity[1]='MASK'\n",
    "        id2entity[2]='CLS'\n",
    "        entity2id = {v:k for k,v in id2entity.items()}\n",
    "        uniid2entity = {v:k.split('\\t')[0].split('^^')[0].strip('\"').replace('\\\\\"','\"') for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_UnifiedNoKGenc/unified_node').items()}\n",
    "        entity2uniid = {v:k for k,v in uniid2entity.items()}\n",
    "        id2uniid = {k:entity2uniid[v] for k,v in id2entity.items() if v in entity2uniid}\n",
    "        db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['rc_index', 'label_mask']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'NoKGenc':\n",
    "                x, y = corrupt_input_and_generate_label(inputs=db['input'][in_db_idx],mode='s',id2entity=id2entity, entity2id=entity2id, code_frequency=code_frequency)\n",
    "                sample = {\n",
    "                    'input': x,\n",
    "                    'label': y,\n",
    "                } if x is not None else None\n",
    "                if sample is not None:\n",
    "                    for k in db_new:\n",
    "                        if k in sample:\n",
    "                            db_new[k].append(sample[k].tolist())\n",
    "                        else:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                global_sample.append(sample)\n",
    "            else:\n",
    "                if global_sample[idx] is not None:\n",
    "                    sample = {k:global_sample[idx][k].copy() for k in global_sample[idx]}\n",
    "                    actual_input = np.array(db['input'][in_db_idx].copy())\n",
    "                    # Convert Non-unified sample to unifieid sample\n",
    "                    if 'Unified' in db_type:\n",
    "                        living_ids = sample['input'][sample['label']]\n",
    "                        convertable_ids = np.array([True if living_id in id2uniid else False for living_id in living_ids])\n",
    "                        sample['label'][sample['label']==True] = convertable_ids\n",
    "                        actual_input[sample['label']] = np.array([id2uniid[x] for x in living_ids[convertable_ids]])\n",
    "                    for k in db_new:\n",
    "                        if k not in sample:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(sample['label'].tolist())\n",
    "                    if 'Unified' in db_type:\n",
    "                        db_new['input'].append(actual_input.tolist())\n",
    "                    else:\n",
    "                        db_new['input'].append(sample['input'].tolist())\n",
    "                            \n",
    "            idx += 1\n",
    "        print('*'*50)\n",
    "        print([uniid2entity[x] if 'Unified' in db_type else id2entity[x] for x in np.array(db['input'][0])[np.where(np.array(db_new['label'][0])==True)]])\n",
    "        print([uniid2entity[x] if 'Unified' in db_type else id2entity[x] for x in np.array(db_new['input'][0])[np.where(np.array(db_new['label'][0])==True)]])\n",
    "        print(np.where(np.array(db_new['label'][0])==True))\n",
    "        print('-'*50)\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        print(list(db_new.keys()))\n",
    "        print('*'*50)\n",
    "        os.makedirs(f'../gtx/data/ed/{db_name}_{size}/{db_name}_{db_type}/{split}', exist_ok=True)\n",
    "        torch.save(db_new,f'../gtx/data/ed/{db_name}_{size}/{db_name}_{db_type}/{split}/db')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1. Convert DB for ErrorDetection for Rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "db_name = 'px'\n",
    "size = 1000\n",
    "DB_type = ['UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare essential files\n",
    "NUM_SPECIAL_TOKENS = 3\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "id2entity[0]='PAD'\n",
    "id2entity[1]='MASK'\n",
    "id2entity[2]='CLS'\n",
    "entity2id = {v:k for k,v in id2entity.items()}\n",
    "uniid2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_UnifiedNoKGenc/unified_node').items()}\n",
    "entity2uniid = {v:k for k,v in uniid2entity.items()}\n",
    "uniid2id = {k:entity2id[v] for k,v in uniid2entity.items() if v in entity2id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjpark/miniconda3/envs/sjpark/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/sjpark/miniconda3/envs/sjpark/lib/python3.7/site-packages/pandas/core/dtypes/common.py:1776: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  npdtype = np.dtype(dtype)\n"
     ]
    }
   ],
   "source": [
    "px = pd.read_csv('../gtx/data/mimic_data/PRESCRIPTIONS.csv')\n",
    "px['DOSE'] = px.apply(lambda row: str(row[14])+str(row[15]), axis=1)\n",
    "px_dict = px.to_dict()\n",
    "CATEGORIES = ['DRUG_TYPE', 'DRUG', 'ROUTE', 'FORMULARY_DRUG_CD', 'DOSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxcat2ids = {k:[f'\"{str(x).lower()}\"' for x in list(set(px_dict[k].values()))] for k in CATEGORIES}\n",
    "id2pxcat = dict()\n",
    "for k in pxcat2ids:\n",
    "    pxcat2ids[k] = [entity2uniid[x] for x in pxcat2ids[k] if x in entity2uniid]\n",
    "    for v in pxcat2ids[k]:\n",
    "        id2pxcat[v]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_type = 'UnifiedNoKGenc'\n",
    "uniid2entity = {v:k.split('\\t')[0].split('^^')[0].strip('\"').replace('\\\\\"','\"') for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_UnifiedNoKGenc/unified_node').items()}\n",
    "px2freq = {k:{v:0 for v in vl} for k,vl in pxcat2ids.items()}\n",
    "# for k,vl in pxcat2ids.items()\n",
    "for split in SPLIT:\n",
    "    db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "    for idx in range(len(db['input'])):\n",
    "        ids = [x for x in db['input'][idx] if x>7]\n",
    "        for _id in ids:\n",
    "            if _id in id2pxcat:\n",
    "                px2freq[id2pxcat[_id]][_id]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "where() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (Tensor condition)\n * (Tensor condition, Tensor input, Tensor other)\n * (Tensor condition, Number self, Tensor other)\n * (Tensor condition, Tensor input, Number other)\n * (Tensor condition, Number self, Number other)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-11740c0d5333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: where() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (Tensor condition)\n * (Tensor condition, Tensor input, Tensor other)\n * (Tensor condition, Number self, Tensor other)\n * (Tensor condition, Tensor input, Number other)\n * (Tensor condition, Number self, Number other)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "label = torch.tensor([[1,1,0,0],[0,0,0,1],[1,1,1,1]])\n",
    "torch.where(label==1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_input_and_generate_label(inputs, literal_pos, pxcat2ids, id2pxcat):\n",
    "#     try:\n",
    "    corruptable_idx = np.array([i for i,(x,p) in enumerate(zip(inputs, literal_pos)) if (p==1) and (x in id2pxcat)])\n",
    "    inputs=np.array(inputs)\n",
    "    inputs_ori = inputs.copy()\n",
    "    corruption_targets_idx = choice(corruptable_idx,max(int(len(corruptable_idx)*0.2),1))\n",
    "    for corruption_target_idx in corruption_targets_idx:\n",
    "        corruption_target_id = inputs[corruption_target_idx]\n",
    "        pxcat = id2pxcat[corruption_target_id]\n",
    "        candidate = np.array(pxcat2ids[pxcat])\n",
    "        candidate[candidate!=corruption_target_id]\n",
    "        corrupted_target_id = choice(candidate)\n",
    "        inputs[corruption_target_idx] = corrupted_target_id\n",
    "\n",
    "    labels = ~(inputs_ori==inputs)\n",
    "#     except:\n",
    "#         inputs = None\n",
    "#         labels = None\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnifiedNoKGenc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23663it [03:21, 117.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "['dexa2', '325-650mg', 'po/ng', '100ml', 'folic acid', 'iv', 'main', '2g', 'po', 'main', 'levetiracetam', 'iv', '500mg', '100mg', 'phenytoin sodium', 'ns250', 'calcg2/100ns', '2-4g', 'po/ng', 'base', '100mg', '0.9% sodium chloride', 'infl0.5lf', 'im', '1000mg', 'phenytoin sodium', '1mg', 'base', 'iv', 'naclflush', 'hydralazine', '10mg', 'iv drip', 'nitroprusside sodium', 'ns100', 'base', '4mg', 'dexa4i', 'cefa10i', '1syr', 'syrg1', '2gm', 'base', 'main', '0.9% sodium chloride', '1-2bag']\n",
      "['lido45i', '60-80meq', 'subcut', '167ml', 'ceftaroline', 'os', 'base', '7400unit', 'interspace', 'base', 'lotensin', 'periphnerve', '20-meq', '1mcg', '*nf*  loteprednol 0.2% ophth', 'phen10es', 'ambi5', '400ml', 'irr', 'main', '275mg', 'citric acid/sodium citrate', 'pseud30l', 'intrapericardial', '1-2loz', 'sodium citrate/citric acid', '6200unit', 'main', 'td', 'cloz25', 'flovent hfa', '0.124-0.5mg', 'po/ng', 'vitamin c', 'proc5l', 'main', '1310mg', 'dex50sy', 'prop40', '250-300unit', 'phyt5', '7.8mg', 'additive', 'base', 'medrol (pak)', '1-1.75mg']\n",
      "(array([ 63,  68,  70,  79,  82,  87,  95,  97, 118, 126, 127, 129, 131,\n",
      "       133, 136, 137, 142, 143, 155, 168, 177, 199, 201, 203, 216, 219,\n",
      "       223, 235, 238, 242, 245, 247, 248, 251, 258, 262, 264, 266, 268,\n",
      "       276, 278, 312, 318, 321, 326, 329]),)\n",
      "--------------------------------------------------\n",
      "px,1000,train 23663\n",
      "['input', 'label', 'text']\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:08, 116.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "['main', 'base', 'potassium phosphate', 'iv', 'morphine sulfate', 'ns', 'ns250', 'main', 'morpconc', 'tp', '1appl', 'vancomycin', 'main', '2.5mg', 'olan5dtb', 'po', 'main', '0.25-0.5mg', 'kcl20/1000d51/2ns', '50ml', 'iso-osmotic dextrose', 'base', 'iv', 'asa81', 'd5 1/2ns', 'main', 'main', '5-10mg', 'infv', 'sw', 'sw50', 'iv', 'metrbase', 'chlorhexidine gluconate 0.12% oral rinse', 'oral', '1000ml', 'base', '0.25-0.5mg', 'furo40i', 'naclflush', 'iv', '250ml', 'lidocaine jelly 2% (urojet)', 'iv', 'cefazolin', 'iv', 'sw', '650mg', 'sc', 'po', 'iv', 'captopril', 'capt125', 'main', '6.25mg', 'levobase3', 'haloperidol', 'sw', 'levofloxacin', 'iv', 'oxycodone liquid', '0.5-1mg', 'oxyco5l', 'ipra2h', 'main', 'albu5500', '2gm', 'haloperidol', '25mg', 'metoprolol', 'iv', '20mg', 'potassium chl 20 meq / 1000 ml d5 1/2 ns', 'pantoprazole sodium', 'vancobase', 'famo20pm', 'iv', 'sw', 'ns', 'iv', '250ml', 'iv', 'sime80', 'furo40i', 'furosemide', 'iv', 'main', 'base', '50ml', 'isonacl', 'morphine sulfate', '2mg', '650mg', 'main', 'meto5i', 'iv', 'furo40i', 'iv drip', '200mg', 'docusate sodium (liquid)']\n",
      "['additive', 'additive', 'donepezil 5 mg or placebo', 'ia', 'viokase 8', 'ed', 'nyst5l', 'base', 'cata2', 'io', '4-5gm', 'osmoglyn', 'base', '20-50mg', 'furo100i', 'as', 'base', '1.4mg', 'teraz1', '165mg', 'lindane shampoo', 'additive', 'enteral tube only ? not oral', '1/2ns250i', 'interferon beta-1b', 'additive', 'base', '1bulk', 'letr2.5', 'maxalt', 'clop7.5l', 'left eye', 'fen1250pca', 'morphine sulfate (syringe)', 'og', '225mg', 'additive', '2000-4500unit', 'leup22.5i', 'creon12', 'ij', '8million units', 'zocor', 'nu', 'alfuzosin', 'j tube', 'namenda', '12.5-2mg', 'et', 'sc', 'po or enteral tube', 'azilect', 'atro4i', 'additive', '4pkt', 'heppremix', 'sanctura xr', 'pravastatin', 'ceftazidime', 'os', 'lamictal xr', '0tube', 'levo200i', 'morp1-5', 'additive', 'morp4-10', '12ml', 'levothyroxine', '4-6inh', 'fosphenytoin sodium', 'neb', '4800-9500unit', 'famotidine', 'butalbital-aspirin-caffeine', 'kcl20/1000d5ns', 'd20w250i', 'in', 'serevent diskus', 'ou', 'ivt', '125mg', 'pr', 'domepkt', 'kaye15p', 'sterile water for irrigation', 'ed', 'base', 'additive', '0.12ml', 'navel10', 'lithium carbonate', '175ml', '5-6gm', 'additive', 'indclof20i', 'left eye', 'ns20syr', 'j tube', '95unit', 'povidone iodine 1/2 strength']\n",
      "(array([118, 124, 132, 140, 146, 150, 152, 166, 168, 178, 180, 188, 193,\n",
      "       194, 195, 200, 202, 207, 210, 222, 224, 225, 226, 233, 241, 258,\n",
      "       269, 280, 287, 292, 294, 299, 301, 307, 310, 313, 315, 327, 331,\n",
      "       338, 340, 347, 355, 359, 361, 382, 383, 387, 404, 410, 412, 418,\n",
      "       419, 421, 422, 428, 432, 434, 444, 449, 453, 464, 470, 472, 474,\n",
      "       494, 501, 516, 536, 537, 556, 564, 569, 577, 583, 590, 605, 612,\n",
      "       616, 618, 622, 626, 635, 638, 639, 641, 644, 650, 653, 658, 659,\n",
      "       662, 670, 672, 675, 678, 683, 686, 689, 694]),)\n",
      "--------------------------------------------------\n",
      "px,1000,valid 1000\n",
      "['input', 'label', 'text']\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:08, 121.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "['main', 'potassium chloride', 'main', 'dolasetron mesylate', 'lr', 'aten50', 'hydromorphone', 'magnesium sulfate', 'main', 'base', 'po', '1mg', 'sodium chloride 0.9%  flush', 'tp', '0.5-2mg', 'hydromorphone', 'iv', 'heparin flush cvl  (100 units/ml)', 'iv', 'artificial tear ointment', 'arti3.5o', 'miconazole powder 2%', '1appl', 'morp4-6', '40meq', '250ml', 'metrbase', 'iv', 'metronidazole', 'base', 'iv', 'sw', 'potassium chloride', 'pant40', '200mg', 'iv', 'base', '500ml', 'metoprolol', 'base', 'pantoprazole sodium', '40mg', 'ns500', '500ml', 'main']\n",
      "['additive', 'saquinavir (invirase) cap', 'additive', 'humalog', 'albute', 'augm1.25l', 'cromolyn', 'methad', 'base', 'additive', 'po/ng', '500unit', 'fat emulsion 20%', 'td', '9900mg', 'rizatriptan', 'io', 'iron dextran', 'po/og', 'levetiracetam', 'quet50xr', 'lactic acid 12% lotion', '210ml', 'clon2p', '300-500unit', '11ml', 'reopi', 'periphnerve', 'flutamide', 'additive', 'po or enteral tube', 'venlafaxine xr', 'sanctura', 'bupi0.1epd', '228ml', 'ed', 'main', '500-50puff', 'loteprednol etabonate', 'additive', 'iron', '1bags', 'betaval.1/15o', '17,000unit', 'additive']\n",
      "(array([ 54,  62,  72,  75,  76,  86,  94, 101, 105, 118, 120, 124, 127,\n",
      "       135, 144, 145, 146, 151, 153, 167, 170, 175, 177, 185, 197, 200,\n",
      "       225, 231, 232, 238, 239, 243, 251, 256, 269, 275, 278, 281, 285,\n",
      "       302, 308, 311, 314, 316, 333]),)\n",
      "--------------------------------------------------\n",
      "px,1000,test 1000\n",
      "['input', 'label', 'text']\n",
      "**************************************************\n",
      "UnifiedUniKGenc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23663it [00:02, 9979.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "['dexa2', '325-650mg', 'po/ng', '100ml', 'folic acid', 'iv', 'main', '2g', 'po', 'main', 'levetiracetam', 'iv', '500mg', '100mg', 'phenytoin sodium', 'ns250', 'calcg2/100ns', '2-4g', 'po/ng', 'base', '100mg', '0.9% sodium chloride', 'infl0.5lf', 'im', '1000mg', 'phenytoin sodium', '1mg', 'base', 'iv', 'naclflush', 'hydralazine', '10mg', 'iv drip', 'nitroprusside sodium', 'ns100', 'base', '4mg', 'dexa4i', 'cefa10i', '1syr', 'syrg1', '2gm', 'base', 'main', '0.9% sodium chloride', '1-2bag']\n",
      "['lido45i', '60-80meq', 'subcut', '167ml', 'ceftaroline', 'os', 'base', '7400unit', 'interspace', 'base', 'lotensin', 'periphnerve', '20-meq', '1mcg', '*nf*  loteprednol 0.2% ophth', 'phen10es', 'ambi5', '400ml', 'irr', 'main', '275mg', 'citric acid/sodium citrate', 'pseud30l', 'intrapericardial', '1-2loz', 'sodium citrate/citric acid', '6200unit', 'main', 'td', 'cloz25', 'flovent hfa', '0.124-0.5mg', 'po/ng', 'vitamin c', 'proc5l', 'main', '1310mg', 'dex50sy', 'prop40', '250-300unit', 'phyt5', '7.8mg', 'additive', 'base', 'medrol (pak)', '1-1.75mg']\n",
      "(array([ 63,  68,  70,  79,  82,  87,  95,  97, 118, 126, 127, 129, 131,\n",
      "       133, 136, 137, 142, 143, 155, 168, 177, 199, 201, 203, 216, 219,\n",
      "       223, 235, 238, 242, 245, 247, 248, 251, 258, 262, 264, 266, 268,\n",
      "       276, 278, 312, 318, 321, 326, 329]),)\n",
      "--------------------------------------------------\n",
      "px,1000,train 23663\n",
      "['input', 'mask', 'label', 'text']\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 13467.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "['main', 'base', 'potassium phosphate', 'iv', 'morphine sulfate', 'ns', 'ns250', 'main', 'morpconc', 'tp', '1appl', 'vancomycin', 'main', '2.5mg', 'olan5dtb', 'po', 'main', '0.25-0.5mg', 'kcl20/1000d51/2ns', '50ml', 'iso-osmotic dextrose', 'base', 'iv', 'asa81', 'd5 1/2ns', 'main', 'main', '5-10mg', 'infv', 'sw', 'sw50', 'iv', 'metrbase', 'chlorhexidine gluconate 0.12% oral rinse', 'oral', '1000ml', 'base', '0.25-0.5mg', 'furo40i', 'naclflush', 'iv', '250ml', 'lidocaine jelly 2% (urojet)', 'iv', 'cefazolin', 'iv', 'sw', '650mg', 'sc', 'po', 'iv', 'captopril', 'capt125', 'main', '6.25mg', 'levobase3', 'haloperidol', 'sw', 'levofloxacin', 'iv', 'oxycodone liquid', '0.5-1mg', 'oxyco5l', 'ipra2h', 'main', 'albu5500', '2gm', 'haloperidol', '25mg', 'metoprolol', 'iv', '20mg', 'potassium chl 20 meq / 1000 ml d5 1/2 ns', 'pantoprazole sodium', 'vancobase', 'famo20pm', 'iv', 'sw', 'ns', 'iv', '250ml', 'iv', 'sime80', 'furo40i', 'furosemide', 'iv', 'main', 'base', '50ml', 'isonacl', 'morphine sulfate', '2mg', '650mg', 'main', 'meto5i', 'iv', 'furo40i', 'iv drip', '200mg', 'docusate sodium (liquid)']\n",
      "['additive', 'additive', 'donepezil 5 mg or placebo', 'ia', 'viokase 8', 'ed', 'nyst5l', 'base', 'cata2', 'io', '4-5gm', 'osmoglyn', 'base', '20-50mg', 'furo100i', 'as', 'base', '1.4mg', 'teraz1', '165mg', 'lindane shampoo', 'additive', 'enteral tube only ? not oral', '1/2ns250i', 'interferon beta-1b', 'additive', 'base', '1bulk', 'letr2.5', 'maxalt', 'clop7.5l', 'left eye', 'fen1250pca', 'morphine sulfate (syringe)', 'og', '225mg', 'additive', '2000-4500unit', 'leup22.5i', 'creon12', 'ij', '8million units', 'zocor', 'nu', 'alfuzosin', 'j tube', 'namenda', '12.5-2mg', 'et', 'sc', 'po or enteral tube', 'azilect', 'atro4i', 'additive', '4pkt', 'heppremix', 'sanctura xr', 'pravastatin', 'ceftazidime', 'os', 'lamictal xr', '0tube', 'levo200i', 'morp1-5', 'additive', 'morp4-10', '12ml', 'levothyroxine', '4-6inh', 'fosphenytoin sodium', 'neb', '4800-9500unit', 'famotidine', 'butalbital-aspirin-caffeine', 'kcl20/1000d5ns', 'd20w250i', 'in', 'serevent diskus', 'ou', 'ivt', '125mg', 'pr', 'domepkt', 'kaye15p', 'sterile water for irrigation', 'ed', 'base', 'additive', '0.12ml', 'navel10', 'lithium carbonate', '175ml', '5-6gm', 'additive', 'indclof20i', 'left eye', 'ns20syr', 'j tube', '95unit', 'povidone iodine 1/2 strength']\n",
      "(array([118, 124, 132, 140, 146, 150, 152, 166, 168, 178, 180, 188, 193,\n",
      "       194, 195, 200, 202, 207, 210, 222, 224, 225, 226, 233, 241, 258,\n",
      "       269, 280, 287, 292, 294, 299, 301, 307, 310, 313, 315, 327, 331,\n",
      "       338, 340, 347, 355, 359, 361, 382, 383, 387, 404, 410, 412, 418,\n",
      "       419, 421, 422, 428, 432, 434, 444, 449, 453, 464, 470, 472, 474,\n",
      "       494, 501, 516, 536, 537, 556, 564, 569, 577, 583, 590, 605, 612,\n",
      "       616, 618, 622, 626, 635, 638, 639, 641, 644, 650, 653, 658, 659,\n",
      "       662, 670, 672, 675, 678, 683, 686, 689, 694]),)\n",
      "--------------------------------------------------\n",
      "px,1000,valid 1000\n",
      "['input', 'mask', 'label', 'text']\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 14314.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "['main', 'potassium chloride', 'main', 'dolasetron mesylate', 'lr', 'aten50', 'hydromorphone', 'magnesium sulfate', 'main', 'base', 'po', '1mg', 'sodium chloride 0.9%  flush', 'tp', '0.5-2mg', 'hydromorphone', 'iv', 'heparin flush cvl  (100 units/ml)', 'iv', 'artificial tear ointment', 'arti3.5o', 'miconazole powder 2%', '1appl', 'morp4-6', '40meq', '250ml', 'metrbase', 'iv', 'metronidazole', 'base', 'iv', 'sw', 'potassium chloride', 'pant40', '200mg', 'iv', 'base', '500ml', 'metoprolol', 'base', 'pantoprazole sodium', '40mg', 'ns500', '500ml', 'main']\n",
      "['additive', 'saquinavir (invirase) cap', 'additive', 'humalog', 'albute', 'augm1.25l', 'cromolyn', 'methad', 'base', 'additive', 'po/ng', '500unit', 'fat emulsion 20%', 'td', '9900mg', 'rizatriptan', 'io', 'iron dextran', 'po/og', 'levetiracetam', 'quet50xr', 'lactic acid 12% lotion', '210ml', 'clon2p', '300-500unit', '11ml', 'reopi', 'periphnerve', 'flutamide', 'additive', 'po or enteral tube', 'venlafaxine xr', 'sanctura', 'bupi0.1epd', '228ml', 'ed', 'main', '500-50puff', 'loteprednol etabonate', 'additive', 'iron', '1bags', 'betaval.1/15o', '17,000unit', 'additive']\n",
      "(array([ 54,  62,  72,  75,  76,  86,  94, 101, 105, 118, 120, 124, 127,\n",
      "       135, 144, 145, 146, 151, 153, 167, 170, 175, 177, 185, 197, 200,\n",
      "       225, 231, 232, 238, 239, 243, 251, 256, 269, 275, 278, 281, 285,\n",
      "       302, 308, 311, 314, 316, 333]),)\n",
      "--------------------------------------------------\n",
      "px,1000,test 1000\n",
      "['input', 'mask', 'label', 'text']\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for db_type in DB_type:\n",
    "    if db_type == 'UnifiedNoKGenc':\n",
    "        global_sample = list()\n",
    "    print(db_type)\n",
    "    \n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['rc_index', 'label_mask']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'UnifiedNoKGenc':\n",
    "                x, y = corrupt_input_and_generate_label(inputs=db['input'][in_db_idx],literal_pos=db['label_mask'][in_db_idx],pxcat2ids=pxcat2ids,id2pxcat=id2pxcat)\n",
    "                sample = {\n",
    "                    'input': x,\n",
    "                    'label': y,\n",
    "                } if x is not None else None\n",
    "                if sample is not None:\n",
    "                    for k in db_new:\n",
    "                        if k in sample:\n",
    "                            db_new[k].append(sample[k].tolist())\n",
    "                        else:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                global_sample.append(sample)\n",
    "            else:\n",
    "                if global_sample[idx] is not None:\n",
    "                    sample = {k:global_sample[idx][k].copy() for k in global_sample[idx]}\n",
    "                    actual_input = np.array(db['input'][in_db_idx].copy())\n",
    "                    # Convert Non-unified sample to unifieid sample\n",
    "                    if 'Unified' not in db_type:\n",
    "                        living_ids = sample['input'][sample['label']]\n",
    "                        actual_input[sample['label']] = np.array([uniid2id[x] for x in living_ids])\n",
    "                    for k in db_new:\n",
    "                        if k not in sample:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(sample['label'].tolist())\n",
    "                    if 'Unified' not in db_type:\n",
    "                        db_new['input'].append(actual_input.tolist())\n",
    "                    else:\n",
    "                        db_new['input'].append(sample['input'].tolist())\n",
    "                            \n",
    "            idx += 1\n",
    "        print('*'*50)\n",
    "        print([uniid2entity[x] if 'Unified' in db_type else id2entity[x] for x in np.array(db['input'][0])[np.where(np.array(db_new['label'][0])==True)]])\n",
    "        print([uniid2entity[x] if 'Unified' in db_type else id2entity[x] for x in np.array(db_new['input'][0])[np.where(np.array(db_new['label'][0])==True)]])\n",
    "        print(np.where(np.array(db_new['label'][0])==True))\n",
    "        print('-'*50)\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        print(list(db_new.keys()))\n",
    "        print('*'*50)\n",
    "        os.makedirs(f'../gtx/data/ed/{db_name}_{size}/{db_name}_{db_type}/{split}', exist_ok=True)\n",
    "        torch.save(db_new,f'../gtx/data/ed/{db_name}_{size}/{db_name}_{db_type}/{split}/db')               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modify Pretraining Input for KnowMix strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "db_name = 'px'\n",
    "size = 1000\n",
    "DB_type = ['UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 23663/23663 [00:02<00:00, 8721.40it/s]\n",
      "100%|| 1000/1000 [00:00<00:00, 10488.70it/s]\n",
      "100%|| 1000/1000 [00:00<00:00, 9765.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 23663/23663 [00:00<00:00, 721066.93it/s]\n",
      "100%|| 1000/1000 [00:00<00:00, 191529.48it/s]\n",
      "100%|| 1000/1000 [00:00<00:00, 481163.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9169\n"
     ]
    }
   ],
   "source": [
    "know_list = list()\n",
    "for db_type in DB_type:\n",
    "    global_idx=0\n",
    "    # Prepare essential files\n",
    "    if \"Unified\" not in db_type:\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2label = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "        label2id = {v:k for k,v in id2label.items()}\n",
    "        id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "        label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    else:\n",
    "        id2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/unified_node').items()}\n",
    "    for split in SPLIT:\n",
    "        db = torch.load(f'../gtx/data/ed/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "        db_new = {k:list() for k in db}\n",
    "        db_new['knowledge'] = list()\n",
    "        for idx in trange(len(db['input'])):\n",
    "            for k in db:\n",
    "                db_new[k].append(db[k][idx])\n",
    "            if db_type == 'UnifiedNoKGenc':\n",
    "                desc = [id2entity[x].replace('\"','') if x>5 else \"\" for x in db_new['input'][idx]]\n",
    "                know_list.append(desc)\n",
    "                db_new['knowledge'].append(desc)\n",
    "            else:\n",
    "                db_new['knowledge'].append(know_list[global_idx])\n",
    "            global_idx += 1\n",
    "        os.makedirs(f'../gtx/data/ed/knowmix/{db_name}_{size}/{db_name}_{db_type}/{split}', exist_ok=True)\n",
    "        torch.save(db_new,f'../gtx/data/ed/knowmix/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "    id2desc = {k:v.split('^^')[0].replace('\"','') if '^^' in v else \"\" for k,v in id2entity.items()}\n",
    "    print(len(id2desc))\n",
    "    torch.save(id2desc,f'../gtx/data/ed/knowmix/{db_name}_{size}/{db_name}_{db_type}/id2desc')\n",
    "#     for k in db:\n",
    "#         print(k,db[k][-1])\n",
    "#     print(\"-\"*30)\n",
    "#     for k in db_new:\n",
    "#         print(db_new[k][-1])\n",
    "#     print(\"-\"*30)\n",
    "                            \n",
    "                \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convert DB for ReAdmPred task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "db_name = 'px'\n",
    "size = 1000\n",
    "DB_type = ['NoKGenc','UniKGenc','UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for db_type in DB_type:\n",
    "    # Prepare essential files\n",
    "    if \"Unified\" not in db_type:\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2label = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "        label2id = {v:k for k,v in id2label.items()}\n",
    "        id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "        label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    else:\n",
    "        id2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/unified_node').items()}\n",
    "\n",
    "    if db_type == 'NoKGenc':\n",
    "        global_label = list()\n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "        readmDB_path = f\"../gtx/data/temporal/readm_{'dxpx' if db_name=='dx,prx' else 'rx'}_{split}.pkl\"\n",
    "        with open(readmDB_path, 'rb') as f:\n",
    "            print(readmDB_path)\n",
    "            raw_readm_db = pkl.load(f)\n",
    "        readm_db = dict()\n",
    "        for sample in raw_readm_db:\n",
    "            hadm_id, label = sample\n",
    "            readm_db[str(hadm_id)] = label\n",
    "        print(\"# samples: \",len(readm_db))\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['label_mask','rc_index']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'NoKGenc':\n",
    "                hadm_id = id2entity[_input[1]].split('/')[-1].replace('>','')\n",
    "                if hadm_id in readm_db:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(readm_db[hadm_id])\n",
    "                    global_label.append(readm_db[hadm_id])\n",
    "                else:\n",
    "                    global_label.append(None)\n",
    "            else:\n",
    "                if global_label[idx] is not None:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(global_label[idx])\n",
    "            idx += 1\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        if \"Unified\" in db_type:\n",
    "            os.makedirs(f'../gtx/data/temporal/readm/{db_name}_{size}/{db_name}_{db_type}/{split}', exist_ok=True)\n",
    "            torch.save(db_new,f'../gtx/data/temporal/readm/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "            print(\"*** Saved! ***\")\n",
    "        print(f\"100th sample\", [id2entity[x] for x in db_new['input'][99][1:30]])\n",
    "        print(f\"100th label\", db_new['label'][99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Convert DB for NextDxPxPred task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "db_name = 'px'\n",
    "size = 1000\n",
    "DB_type = ['NoKGenc','UniKGenc','UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for db_type in DB_type:\n",
    "    # Prepare essential files\n",
    "    if \"Unified\" not in db_type:\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2label = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "        label2id = {v:k for k,v in id2label.items()}\n",
    "        id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "        label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    else:\n",
    "        id2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/unified_node').items()}\n",
    "\n",
    "    if db_type == 'NoKGenc':\n",
    "        global_label = list()\n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "        nextdxDB_path = f\"../gtx/data/temporal/nextdx_{'dxpx' if db_name=='dx,prx' else 'rx'}_{split}.pkl\"\n",
    "        with open(nextdxDB_path, 'rb') as f:\n",
    "            print(nextdxDB_path)\n",
    "            raw_nextdx_db = pkl.load(f)\n",
    "        nextdx_db = dict()\n",
    "        for sample in raw_nextdx_db:\n",
    "            hadm_id, label = str(int(sample[0])), sample[1:]\n",
    "            nextdx_db[hadm_id] = label\n",
    "        print(\"# samples: \",len(nextdx_db))\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['label_mask','rc_index']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'NoKGenc':\n",
    "                hadm_id = id2entity[_input[1]].split('/')[-1].replace('>','')\n",
    "                if hadm_id in nextdx_db:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(nextdx_db[hadm_id])\n",
    "                    global_label.append(nextdx_db[hadm_id])\n",
    "                else:\n",
    "                    global_label.append(None)\n",
    "            else:\n",
    "                if global_label[idx] is not None:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(global_label[idx])\n",
    "            idx += 1\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        if \"Unified\" in db_type:\n",
    "            os.makedirs(f'../gtx/data/temporal/nextdx/{db_name}_{size}/{db_name}_{db_type}/{split}', exist_ok=True)\n",
    "            torch.save(db_new,f'../gtx/data/temporal/nextdx/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "            print(\"*** Saved! ***\")\n",
    "        print(f\"100th sample\", [id2entity[x] for x in db_new['input'][99][100:105]])\n",
    "        print(f\"100th label\", db_new['label'][99][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Convert DB for Mortaility Pred task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "db_name = 'px'\n",
    "size = 1000\n",
    "# db_name = 'dx,prx'\n",
    "# size = 2000\n",
    "DB_type = ['NoKGenc','UniKGenc','UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for db_type in DB_type:\n",
    "    # Prepare essential files\n",
    "    if \"Unified\" not in db_type:\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2label = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "        label2id = {v:k for k,v in id2label.items()}\n",
    "        id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "        label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    else:\n",
    "        id2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/unified_node').items()}\n",
    "\n",
    "    if db_type == 'NoKGenc':\n",
    "        global_label = list()\n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "        death180DB_path = f\"../gtx/data/temporal/death180_{'dxpx' if db_name=='dx,prx' else 'rx'}_{split}.pkl\"\n",
    "        with open(death180DB_path, 'rb') as f:\n",
    "            print(death180DB_path)\n",
    "            raw_death180_db = pkl.load(f)\n",
    "        death180_db = dict()\n",
    "        for sample in raw_death180_db:\n",
    "            hadm_id, label = sample\n",
    "            death180_db[str(hadm_id)] = label\n",
    "        print(\"# samples: \",len(death180_db))\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['label_mask','rc_index']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'NoKGenc':\n",
    "                hadm_id = id2entity[_input[1]].split('/')[-1].replace('>','')\n",
    "                if hadm_id in death180_db:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(death180_db[hadm_id])\n",
    "                    global_label.append(death180_db[hadm_id])\n",
    "                else:\n",
    "                    global_label.append(None)\n",
    "            else:\n",
    "                if global_label[idx] is not None:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(global_label[idx])\n",
    "            idx += 1\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        if \"Unified\" in db_type:\n",
    "            os.makedirs(f'../gtx/data/temporal/death180/{db_name}_{size}/{db_name}_{db_type}/{split}', exist_ok=True)\n",
    "            torch.save(db_new,f'../gtx/data/temporal/death180/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "            print(\"*** Saved! ***\")\n",
    "        print(f\"100th sample\", [id2entity[x] for x in db_new['input'][99][1:5]])\n",
    "        print(f\"100th label\", db_new['label'][99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prepare literal to word dictionary for word initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "db_name = 'px'\n",
    "size = 1000\n",
    "DB_type = ['UnifiedNoKGenc','UnifiedUniKGenc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for db_type in DB_type:\n",
    "    global_idx=0\n",
    "    # Prepare essential files\n",
    "    id2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/unified_node').items()}\n",
    "    desc = [id2entity[x].replace('\"','') if x>5 else \"\" for x in id2entity]\n",
    "    print(len(id2entity))\n",
    "    print(id2entity)\n",
    "    torch.save(desc,f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/lit2word')\n",
    "    torch.save(desc,f'../gtx/data/knowmix/{db_name}_{size}/{db_name}_{db_type}/lit2word')\n",
    "\n",
    "                            \n",
    "                \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT. FIX CLS ATTN MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm, trange\n",
    "db_name = 'dx,prx'\n",
    "size = 2000\n",
    "TASK = [\"\",\"adm/\",\"ed/\",\"readm/\",\"Death30/\"]\n",
    "DB_type = ['UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 23663/23663 [01:21<00:00, 290.31it/s]\n",
      "100%|| 1000/1000 [00:02<00:00, 340.05it/s]\n",
      "100%|| 1000/1000 [00:02<00:00, 339.12it/s]\n",
      "100%|| 19780/19780 [01:02<00:00, 315.23it/s]\n",
      "100%|| 825/825 [00:02<00:00, 341.60it/s]\n",
      "100%|| 837/837 [00:02<00:00, 342.00it/s]\n",
      "100%|| 23663/23663 [01:18<00:00, 302.45it/s]\n",
      "100%|| 1000/1000 [00:02<00:00, 342.87it/s]\n",
      "100%|| 1000/1000 [00:02<00:00, 344.34it/s]\n",
      "100%|| 5858/5858 [00:17<00:00, 327.45it/s]\n",
      "100%|| 265/265 [00:00<00:00, 349.14it/s]\n",
      "100%|| 281/281 [00:00<00:00, 350.77it/s]\n",
      "100%|| 23568/23568 [01:16<00:00, 308.25it/s]\n",
      "100%|| 986/986 [00:02<00:00, 345.94it/s]\n",
      "100%|| 983/983 [00:02<00:00, 344.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for fusion in [True,False]:\n",
    "    for task in TASK:\n",
    "        if fusion:\n",
    "            task = task+'knowmix/'\n",
    "        for db_type in DB_type:\n",
    "            for split in SPLIT:\n",
    "                try:\n",
    "                    db = torch.load(f'../gtx/data/{task}{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "                except:\n",
    "                    continue\n",
    "                db_length = len(db['input'])\n",
    "                for idx in trange(db_length):\n",
    "                    _input, _mask = db['input'][idx], db['mask'][idx]\n",
    "                    new_cls_mask = [1 if x!=0 else 0 for x in _input]\n",
    "                    _mask[0] = torch.tensor(new_cls_mask)\n",
    "                    db['mask'][idx] = _mask\n",
    "                    if sum(db['mask'][idx][0])!= sum([x!=0 for x in _input]):\n",
    "                        raise ValueError()\n",
    "                os.makedirs(f'../gtx/fixed_data/{task}/{db_name}_{size}/{db_name}_{db_type}/{split}', exist_ok=True)\n",
    "                torch.save(db,f'../gtx/fixed_data/{task}/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "                os.remove(f'../gtx/data/{task}/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "                del db\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../gtx/data/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/adm/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/adm/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/adm/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/ed/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/ed/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/ed/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/readm/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/readm/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/readm/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/Death30/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/Death30/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/Death30/knowmix/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n",
      "../gtx/data/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/adm/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/adm/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/adm/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/ed/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/ed/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/ed/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/readm/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/readm/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/readm/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n",
      "../gtx/data/Death30/dx,prx_2000/dx,prx_UnifiedUniKGenc/train/db\n",
      "../gtx/data/Death30/dx,prx_2000/dx,prx_UnifiedUniKGenc/valid/db\n",
      "../gtx/data/Death30/dx,prx_2000/dx,prx_UnifiedUniKGenc/test/db\n"
     ]
    }
   ],
   "source": [
    "for fusion in [True,False]:\n",
    "    for task in TASK:\n",
    "        if fusion:\n",
    "            task = task+'knowmix/'\n",
    "        for db_type in DB_type:\n",
    "            for split in SPLIT:\n",
    "                path = f'../gtx/data/{task}{db_name}_{size}/{db_name}_{db_type}/{split}/db'\n",
    "                print(path)\n",
    "                try:\n",
    "                    db = torch.load(path)\n",
    "                    \n",
    "                except:\n",
    "#                     print(\"fail\")\n",
    "                    continue\n",
    "                \n",
    "                db_length = len(db['input'])\n",
    "                print(db['mask'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knowmix/ UnifiedUniKGenc 2\n",
      "adm/knowmix/ UnifiedUniKGenc 1\n",
      "ed/knowmix/ UnifiedUniKGenc 1\n",
      "readm/knowmix/ UnifiedUniKGenc 1\n",
      "Death30/knowmix/ UnifiedUniKGenc 1\n",
      " UnifiedUniKGenc 4\n",
      "adm/ UnifiedUniKGenc 0\n",
      "ed/ UnifiedUniKGenc 0\n",
      "readm/ UnifiedUniKGenc 0\n",
      "Death30/ UnifiedUniKGenc 0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "for fusion in [True,False]:\n",
    "    for task in TASK:\n",
    "        if fusion:\n",
    "            task = task+'knowmix/'\n",
    "        for db_type in DB_type:\n",
    "            count = 0\n",
    "            try:\n",
    "                shutil.copy(f'../gtx/data/{task}{db_name}_{size}/{db_name}_{db_type}/lit2word',\n",
    "                           f'../gtx/fixed_data/{task}{db_name}_{size}/{db_name}_{db_type}/lit2word')\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                shutil.copy(f'../gtx/data/{task}{db_name}_{size}/{db_name}_{db_type}/id2label',\n",
    "                           f'../gtx/fixed_data/{task}{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                shutil.copy(f'../gtx/data/{task}{db_name}_{size}/{db_name}_{db_type}/node2uninode',\n",
    "                           f'../gtx/fixed_data/{task}{db_name}_{size}/{db_name}_{db_type}/node2uninode')\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                shutil.copy(f'../gtx/data/{task}{db_name}_{size}/{db_name}_{db_type}/unified_node',\n",
    "                           f'../gtx/fixed_data/{task}{db_name}_{size}/{db_name}_{db_type}/unified_node')\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                shutil.copy(f'../gtx/data/{task}{db_name}_{size}/{db_name}_{db_type}/id2desc',\n",
    "                           f'../gtx/fixed_data/{task}{db_name}_{size}/{db_name}_{db_type}/id2desc')\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "            print(task, db_type, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "db_name = 'px'\n",
    "size = 1000\n",
    "db_type = 'UnifiedUniKGenc'\n",
    "split = 'valid'\n",
    "db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "if \"Unified\" not in db_type:\n",
    "    NUM_SPECIAL_TOKENS = 3\n",
    "    id2label = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "    label2id = {v:k for k,v in id2label.items()}\n",
    "    id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "    label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    entity2id = {v:k for k,v in id2entity.items()}\n",
    "else:\n",
    "    id2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/unified_node').items()}\n",
    "    entity2id = {v:k for k,v in id2entity.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id2entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "db_name = 'dx,prx'\n",
    "size = 2000\n",
    "db_type = 'NoKGenc'\n",
    "if \"Unified\" not in db_type:\n",
    "    NUM_SPECIAL_TOKENS = 3\n",
    "    id2label = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/id2label')\n",
    "    label2id = {v:k for k,v in id2label.items()}\n",
    "    id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'../gtx/data/{db_name}','entity2id.txt')).read().splitlines()[1:]}\n",
    "    label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    entity2id = {v:k for k,v in id2entity.items()}\n",
    "else:\n",
    "    id2entity = {v:k.split('\\t')[0].split('^^')[0] for k,v in torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/unified_node').items()}\n",
    "    entity2id = {v:k for k,v in id2entity.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "seen_ids = list()\n",
    "for split in ['train','valid','test']:\n",
    "    db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')\n",
    "    count = 0\n",
    "    with open(f'{db_name}_{split}.txt','w') as f:\n",
    "        for sample in db['input']:\n",
    "            aid = id2entity[sample[1]].split('/')[-1].replace('>','')\n",
    "            if aid in seen_ids and split != \"train\":\n",
    "                print(\"oh my god!\")\n",
    "                print(aid)\n",
    "                count +=1\n",
    "            seen_ids.append(aid)\n",
    "            f.write(aid+'\\n')\n",
    "    print(count)\n",
    "    del db\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_discharge_smmary[df_discharge_smmary['HADM_ID']==167118].values[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('../gtx/data/temporal/icd2int.pkl', 'rb') as f:\n",
    "    icd2int = pkl.load(f)\n",
    "    \n",
    "with open('../gtx/data/temporal/nextdx_dxpx_test.pkl', 'rb') as f:\n",
    "    nextdx_dxpx = pkl.load(f)\n",
    "    \n",
    "with open('../gtx/data/temporal/nextdx_rx_test.pkl', 'rb') as f:\n",
    "    nextdx_rx = pkl.load(f)\n",
    "\n",
    "with open('../gtx/data/temporal/readm_rx_test.pkl', 'rb') as f:\n",
    "    readm_rx = pkl.load(f)\n",
    "    \n",
    "with open('../gtx/data/temporal/readm_dxpx_test.pkl', 'rb') as f:\n",
    "    readm_dxpx = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in nextdx_dxpx:\n",
    "    hadm_id, label = sample[0], sample[1:]\n",
    "print(str(int(hadm_id)))\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "db = torch.load(f'../gtx/data/dx,prx_2000/dx,prx_NoKGenc/train/db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df_diag = pandas.read_csv(\"../gtx/data/DIAGNOSES_ICD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note = pandas.read_csv(\"../gtx/data/DIAGNOSES_ICD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "db_name = 'dx,prx'\n",
    "size = 2000\n",
    "db_type = 'UnifiedNoKGenc'\n",
    "split=\"test\"\n",
    "db = torch.load(f'../gtx/data/{db_name}_{size}/{db_name}_{db_type}/{split}/db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-25d566e277b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_cls_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_cls_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mask'"
     ]
    }
   ],
   "source": [
    "_input, _mask = db['input'][0], db['mask'][0]\n",
    "new_cls_mask = [1 if x!=0 else 0 for x in _input]\n",
    "_mask[0] = torch.tensor(new_cls_mask)\n",
    "db['mask'][0] = _mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(77.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['mask'][0][0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x!=0 for x in db['input'][0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
